{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21625,
     "status": "ok",
     "timestamp": 1666229033656,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "d8DM16is9p93",
    "outputId": "a58a36d1-7525-4bbc-87cb-5694cc88160e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1666229072169,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "ciiLQM_S8av9"
   },
   "outputs": [],
   "source": [
    "###  키워드 분석 클래스 시작\n",
    "class KeywordAnalysis() :\n",
    "    def __init__(self) :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JF6Hvtmc_gYp"
   },
   "outputs": [],
   "source": [
    "# ! pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zkv0ZnQM_uwJ"
   },
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1666229262629,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "C7zG3hTn8lFq"
   },
   "outputs": [],
   "source": [
    "    ### 전처리 모듈\n",
    "    def preproc(self, reviews, custom_stopwords, custom_noun, spellcheck=False) :\n",
    "\n",
    "        print(\"전처리 시작\")\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import re\n",
    "        from ckonlpy.tag import Twitter\n",
    "        from hanspell import spell_checker\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        #  문서군 변수 생성\n",
    "        documents = []\n",
    "\n",
    "        #  기존 불용어 위에 사용자 지정 불용어 적용\n",
    "        with open(\"./data/한국어불용어_new.txt\", \"r\", encoding=\"utf-8\") as f :\n",
    "            stop_words = f.read()\n",
    "        f.close()\n",
    "        stop_words = stop_words.split(\",\")\n",
    "        stop_words.extend(custom_stopwords)\n",
    "\n",
    "        #  사용자 지정 단어 사전 추가\n",
    "        from ckonlpy.tag import Twitter\n",
    "        custom_okt = Twitter()\n",
    "        for n in custom_noun:\n",
    "            custom_okt.add_dictionary(n, \"Noun\")\n",
    "\n",
    "        #  결측치가 존재하는 record 제거\n",
    "        reviews.dropna(inplace=True)\n",
    "\n",
    "        #  의미있는한글(ㅋㅋ, ㅎㅎ이런거 말고), 알파벳, 숫자, 띄어쓰기 제외한 글자 삭제\n",
    "        for idx, comment in enumerate(reviews[\"comment\"]) :\n",
    "            comment = re.sub(r\"[^가-힣a-zA-Z0-9 ]\", \"\", comment)\n",
    "            reviews.iloc[idx,2] = comment\n",
    "\n",
    "        #  띄어쓰기, 맞춤법 자동 교정 (주의!! 엄청 오래걸림!!!)\n",
    "        if spellcheck == True :\n",
    "            from hanspell import spell_checker\n",
    "            for comment in tqdm(reviews[\"comment\"]) :\n",
    "                comment = spell_checker.check(comment).checked\n",
    "                reviews.iloc[idx,2] = comment\n",
    "\n",
    "        #  불용어 제거, 한글자 단어 제거\n",
    "        for comment in reviews[\"comment\"] :\n",
    "            document = \"\"\n",
    "            words = custom_okt.nouns(comment)\n",
    "            for word in words :\n",
    "                if (len(word) >= 2) or (word in custom_noun) :\n",
    "                    if word not in stop_words :\n",
    "                        document += word + \" \"\n",
    "            document = document.rstrip()\n",
    "            documents.append(document)\n",
    "\n",
    "        print(\"전처리 완료, 문서군 생성\\n\")\n",
    "        #  문서군 변수 리턴\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1666229267099,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "kKULcvGf8roq"
   },
   "outputs": [],
   "source": [
    "    ###  키워드 추출 모듈\n",
    "\n",
    "    #  빈도수 기반 키워드를 추출하는 모듈\n",
    "    def cnt(self, documents, filename=\"\", cnt_th=10) :\n",
    "\n",
    "        print(\"빈도수 기반 키워드 추출 시작\")\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from collections import Counter\n",
    "\n",
    "        #  문서군 내의 모든 문서들을 하나의 문서로 모음\n",
    "        documents_sum = []\n",
    "        for document in documents :\n",
    "            words = document.split(\" \")\n",
    "            if words[0] != \"\" :   # 공백이 카운트되는것 방지용\n",
    "                documents_sum.append(words)\n",
    "        documents_sum = sum(documents_sum, [])\n",
    "\n",
    "        #  단어들의 빈도수 세기\n",
    "        counter = Counter(documents_sum)\n",
    "        vocab_sorted = sorted(counter.items(),  key=lambda x : x[1], reverse=True)\n",
    "\n",
    "        #  빈도수 cnt_th 이상인 단어만 추출\n",
    "        vocab_result = []\n",
    "        for vocab in vocab_sorted :\n",
    "            if vocab[1] >= cnt_th :\n",
    "                vocab_result.append(vocab)\n",
    "\n",
    "        #  변수 vocab_result를 데이터프레임으로 만들기\n",
    "        data = np.array([None for _ in range(len(vocab_result)*2)])\n",
    "        data = data.reshape((len(vocab_result),2))\n",
    "        result_df = pd.DataFrame(data, columns=[\"noun\", \"count\"])\n",
    "\n",
    "        for idx, vocab in enumerate(vocab_result) :\n",
    "            result_df.iloc[idx, 0] = vocab[0]\n",
    "            result_df.iloc[idx, 1] = vocab[1]\n",
    "\n",
    "        #  지정한 파일명으로 csv 파일 저장\n",
    "        #  (파일명을 지정하지 않으면 파일로 저장되지 않고 리턴만 함.)\n",
    "        if filename != \"\" :\n",
    "            result_df.to_csv(\"{}.csv\".format(filename), index=False, sep=\",\", encoding=\"utf-8\")\n",
    "            print(\"csv파일 저장 완료\")\n",
    "\n",
    "        print(\"빈도수 기반 키워드 추출 완료\\n\")\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1666229269669,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "MHfrnO7D8xsZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "    #  TF-IDF 방식으로 키워드를 추출하는 모듈\n",
    "    def tfidf(self, documents, filename=\"\") :\n",
    "\n",
    "        print(\"TF-IDF 기반 키워드 추출 시작\")\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "        #  문서군의 TF-IDF 행렬 생성\n",
    "        tfidf_vectorizer = TfidfVectorizer()\n",
    "        tfidf_mat = tfidf_vectorizer.fit_transform(documents)\n",
    "        tfidf_mat = pd.DataFrame(\n",
    "            tfidf_mat.toarray(), columns = tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "        #  각 문서별로 TF-IDF 값이 가장 높은 단어만 선별\n",
    "        max_mat = pd.DataFrame(columns=[\"word\", \"tfidf\"])\n",
    "        max_mat[\"tfidf\"] = tfidf_mat.max(axis=\"columns\")\n",
    "        max_mat[\"word\"] = tfidf_mat.idxmax(axis=\"columns\")\n",
    "\n",
    "        #  각 단어들에 대한 TF-IDF값들 통계\n",
    "        #  word : 단어\n",
    "        #  freq_count : 해당 단어의 TF-IDF값이 가장 높게 나온 문서 개수\n",
    "        #  tfidf_mean, tfidf_std : 해당 단어의 TF-IDF값들 평균, 표준편차\n",
    "        statistic_mat = pd.DataFrame(\n",
    "            columns=[\"word\", \"freq_count\", \"tfidf_mean\", \"tfidf_std\"])\n",
    "\n",
    "        th = 0.6  # TF-IDF값이 0.6 이상인 단어만 저장\n",
    "        statistic_mat[\"word\"] = list(\n",
    "            max_mat[max_mat[\"tfidf\"]>th].groupby(\"word\")[\"word\"].count().index)\n",
    "\n",
    "        statistic_mat[\"freq_count\"] = list(\n",
    "            max_mat[max_mat[\"tfidf\"]>th].groupby(\"word\")[\"word\"].count())\n",
    "\n",
    "        statistic_mat[\"tfidf_mean\"] = list(\n",
    "            max_mat[max_mat[\"tfidf\"]>th].groupby(\"word\")[\"tfidf\"].mean())\n",
    "\n",
    "        statistic_mat[\"tfidf_std\"] = list(\n",
    "            max_mat[max_mat[\"tfidf\"]>th].groupby(\"word\")[\"tfidf\"].std())\n",
    "\n",
    "        statistic_mat.sort_values(by=\"freq_count\", ascending=False, inplace=True)\n",
    "        statistic_mat = statistic_mat.head(100)  # 빈도수 상위 100개만 저장\n",
    "        statistic_mat.reset_index(drop=True, inplace=True)\n",
    "        statistic_mat = round(statistic_mat, 4)\n",
    "\n",
    "        if filename != \"\" :\n",
    "            statistic_mat.to_csv(\"{}.csv\".format(filename),index=False ,sep=\",\", encoding=\"utf-8\")\n",
    "            print(\"TF-IDF 기반 키워드 csv 파일 저장 완료\")\n",
    "\n",
    "        print(\"TF-IDF 기반 키워드 추출 완료\\n\")\n",
    "        return statistic_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1666229272701,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "gJw1OGz_8zhB"
   },
   "outputs": [],
   "source": [
    "    ### 기타 분류 작업에 필요한 모듈    \n",
    "\n",
    "    #  특정 키워드가 들어있는 문서만 골라내는 모듈\n",
    "    def sel_doc(self, documents, keyword) :\n",
    "\n",
    "        result_documents = []\n",
    "\n",
    "        for document in documents :\n",
    "            if keyword in document :\n",
    "                result_documents.append(document)\n",
    "\n",
    "        return result_documents\n",
    "\n",
    "\n",
    "    #  빈도수 기반 키워드를 추출한것 중 빈도수가 가장 높게 나온것들 10개\n",
    "    def cnt_top10(self, result_df) :\n",
    "\n",
    "        top10_list = []\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        for idx, word in enumerate(result_df[\"noun\"]) :\n",
    "            if idx == 10 : break\n",
    "            top10_list.append(word)\n",
    "            # print(top10_list)\n",
    "\n",
    "        return top10_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1666229275130,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "K8oLj5pW82SC"
   },
   "outputs": [],
   "source": [
    "# 키워드 분석 클래스 끝\n",
    "# ======================================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1666229279135,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "_p7FwiWt82Nh"
   },
   "outputs": [],
   "source": [
    "###  시각화 클래스 시작\n",
    "class Visualization() :\n",
    "    #  키워드 추출 결과 csv파일들을 각종 방식으로 시각화하는 기능들이 들어있음.\n",
    "    def __init__(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666229280461,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "-ZS6I2xY892x"
   },
   "outputs": [],
   "source": [
    "    #  워드클라우드\n",
    "    def wordc(self, csvfile, filename=\"\") :\n",
    "\n",
    "        from wordcloud import WordCloud\n",
    "        import pandas as pd\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        matplotlib.rcParams['figure.figsize'] = (5,5)\n",
    "        matplotlib.rcParams['font.family'] = \"NanumGothic\"\n",
    "        matplotlib.rcParams['font.size'] = 10\n",
    "\n",
    "        wc_source = dict()\n",
    "        for idx in range(len(csvfile)) :\n",
    "            if idx == 100 : break\n",
    "            wc_source[csvfile.iloc[idx, 0]] = csvfile.iloc[idx, 1]\n",
    "\n",
    "        wc = WordCloud(\n",
    "            font_path=\"나눔고딕ExtraBold\",width=400, height=400, scale=2.0, max_font_size=250,\n",
    "            background_color=\"white\", colormap=\"autumn\", random_state = 20)\n",
    "        gen = wc.generate_from_frequencies(wc_source)\n",
    "\n",
    "        plt.title(f\"{filename}\")\n",
    "\n",
    "        wc.to_file(f'시각화_wordcloud_{filename}.png')\n",
    "        plt.imshow(gen)\n",
    "        plt.show()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1666229282951,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "pseEvy4-9A_4"
   },
   "outputs": [],
   "source": [
    "    #  파이차트\n",
    "    def pie(self, csvfile, filename=\"\") :\n",
    "\n",
    "        import pandas as pd\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        matplotlib.rcParams['figure.figsize'] = (5,5)\n",
    "        matplotlib.rcParams['font.family'] = \"NanumGothic\"\n",
    "        matplotlib.rcParams['font.size'] = 10\n",
    "\n",
    "\n",
    "        values = []\n",
    "        labels = []\n",
    "\n",
    "        th = 20   # 상위 20개만\n",
    "        for idx in range(len(csvfile)) :\n",
    "            values.append(csvfile.iloc[idx,1]) # count\n",
    "            labels.append(csvfile.iloc[idx,0]) # 키워드\n",
    "            if idx == th : break\n",
    "        \n",
    "        wedgeprops={'width': 0.7, 'edgecolor': 'w', 'linewidth': 5}\n",
    "        plt.pie(\n",
    "            values, labels=labels,\n",
    "            startangle=90, autopct='%.1f%%', counterclock=False, wedgeprops=wedgeprops)\n",
    "\n",
    "        plt.title(f\"{filename}\")\n",
    "\n",
    "        plt.savefig(f'시각화_piechart_{filename}.png')\n",
    "        plt.show()\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1666229285181,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "tUrZ_-ME9Cop"
   },
   "outputs": [],
   "source": [
    "    #  막대그래프\n",
    "    def bar(self, csvfile, filename=\"\", tfidf=False) :\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        matplotlib.rcParams['figure.figsize'] = (15,6)\n",
    "        matplotlib.rcParams['font.family'] = \"NanumGothic\"\n",
    "        matplotlib.rcParams['font.size'] = 10\n",
    "\n",
    "        th = 40  # 상위 40개만\n",
    "        x = np.arange(th)\n",
    "\n",
    "        values = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in range(len(csvfile)) :\n",
    "            if idx == th : break\n",
    "            values.append(csvfile.iloc[idx,1]) # count\n",
    "            labels.append(csvfile.iloc[idx,0]) # 키워드\n",
    "\n",
    "        plt.bar(\n",
    "            labels, values,\n",
    "            width=0.7)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.title(f\"{filename}\")\n",
    "        plt.xlabel(\"키워드\", labelpad=11)\n",
    "        if tfidf :\n",
    "            plt.ylabel(\"TF-IDF값이 가장 높았던 문서 개수\", labelpad=11)\n",
    "        else :\n",
    "            plt.ylabel(\"전체 문서에서 등장한 회수\", labelpad=11)\n",
    "\n",
    "        plt.savefig(f'시각화_bargraph_{filename}.png')\n",
    "        plt.show()\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1666229287242,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "BllnLaD79ELR"
   },
   "outputs": [],
   "source": [
    "# 시각화 클래스 끝\n",
    "# ======================================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1666229290722,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "b5lsmixN9QoI"
   },
   "outputs": [],
   "source": [
    "### 전역변수부 시작\n",
    "\n",
    "#  내가 직접 지정한 불용어 리스트 작성\n",
    "custom_stopwords = [\n",
    "    \"범죄도시\", \"범죄 도시\", \"범죄\", \"도시\", \"죽지\",\n",
    "    \"늑대사냥\", \"늑대\", \"사냥\",\n",
    "    \"공조\", \"인터네셔날\",\"인터내셔날\",\n",
    "    \"한산\", \"용\", \"용의\", \"출현\", \"용의출현\", \"장군님\", \"장군\",\n",
    "    \"뜨거운피\", \"피\", \"뜨거운\",\n",
    "    \"외계인\", \"외계\", \"인\",\n",
    "    \"영화\", \"무비\", \"영화관\", \"한국영화\",\n",
    "    \"진짜\", \"진자\", \"완전\", \"존나\", \"졸라\", \"느낌\", \"뭔가\", \"제일\", \"개\", \"보고\", \"사람\", \"보지\",\n",
    "    \"어요\", \"오늘\", \"그냥\", \"생각\", \"면서\", \"더니\", \"인적\", \"거지\", \"보기\", \"나름\", \"살짝\",\n",
    "    \"정말\", \"대박\", \"역대\", \"최고\", \"어제\", \"편이\", \"계속\", \"요소\", \"처럼\", \"이나\", \"역시\", \"부분\", \"던데\",\n",
    "    \"스포\", \"개봉\", \"한번\", \"내내\", \"구나\", \"때문\", \"어서\", \"정도\", \"다가\", \"다시\", \"누가\", \"덕분\", \"항상\",\n",
    "    \"봤는데\", \"왔는데\",\n",
    "    \"스포일러가 포함된 감상평입니다.\"\n",
    "]\n",
    "\n",
    "#  ckonlpy에 추가할 단어 지정\n",
    "custom_noun = [\n",
    "\n",
    "    #  [ 범죄도시 ]\n",
    "    \"장이수\", \"강해상\", \"손석구\", \"마석도\", \"마동석\", \"마블리\", \"박지환\", \"윤계상\",\n",
    "    \"2부\", \"2탄\", \"2편\", \"1부\", \"1탄\", \"1편\",\n",
    "    #  * 같은 단어 : 장이수=이수, 마석도=마동석=마블리\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 구씨 : 배우 손석구가 다른 작품에서 맡았던 케릭터 이름\n",
    "\n",
    "    #  [ 늑대사냥 ]\n",
    "    \"서인국\", \"장동윤\", \"박호산\", \"정소민\", \"고창석\", \"성동일\",\n",
    "    \"콘에어\", \"콘 에어\", \"잔인\",\n",
    "    #  * 같은 단어 : 콘에어=콘 에어\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 콘에어 : 니콜라스 케이지 출연 1997년 영화. 늑대사냥과 비슷한 소재를 다루고있음.\n",
    "\n",
    "    #  [ 공조2 ]\n",
    "    \"현빈\", \"유해진\", \"윤아\", \"다니엘 헤니\", \"다니엘헤니\", \"다니엘\", \"헤니\", \"진선규\",\n",
    "    \"장영남\", \"박훈\", \"임성재\", \"림청렬\", \"청렬\", \"강진태\", \"진태\", \"장명준\", \"박소연\",\n",
    "    \"박상위\", \"박민영\",\n",
    "    \"불시착\", \"사랑의 불시착\",\n",
    "    # 같은 단어 : 다니엘헤니=다니엘=헤니, 불시착=사랑의 불시착\n",
    "\n",
    "    #  [ 외계인1부 ]\n",
    "    \"류준열\", \"김우빈\", \"김태리\", \"소지섭\", \"염정아\", \"조우진\", \"이하늬\",\n",
    "    \"무륵\", \"김현중\", \"이안\", \"문도석\", \"흑설\", \"청운\",\n",
    "    \"2부\", \"2탄\", \"2편\", \"1부\", \"1탄\", \"1편\", \"돈\", \"SF\", \"sf\", \"SF물\", \"sf물\",\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 돈 : 한 글자 '돈 아깝다'같은 말이 종종 보임.\n",
    "    #  - SF, sf : 장르명. 한글이 아니라고해서 없애면 안됨.\n",
    "\n",
    "    #  [ 한산 ]\n",
    "    \"박해일\", \"안성기\", \"변요한\", \"손현주\", \"김성규\", \"김성균\", \"김향기\", \"택연\", \"옥택연\",\n",
    "    \"이순신\", \"장군님\", \"장군\", \"어영담\", \"원균\", \"준산\", \"히데요시\", \"도요토미\", \"도요토미 히데요시\",\n",
    "    \"한산도\", \"대첩\", \"한산도 대첩\", \"임진왜란\", \"유키나가\", \"학익진\",\n",
    "    #  * 같은 단어 : 이순신=이순신장군\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 하라 : '전군 출정하라', '선회하라', '발포하라'가 이 영화의 명대사인가봄.\n",
    "\n",
    "    # 뜨거운피 배우명/등장인물명\n",
    "    \"정우\", \"김갑수\", \"최무성\", \"지승현\", \"김해곤\", \"윤지혜\", \"이홍내\", \"정호빈\"\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 짱구 : 배우 정우가 다른 작품에서 맡았던 케릭터 이름\n",
    "\n",
    "    ]\n",
    "\n",
    "#  클래스 생성자 만들기\n",
    "keywordAnalysis = KeywordAnalysis()\n",
    "visualization = Visualization()\n",
    "\n",
    "### 전역변수부 끝\n",
    "# ======================================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1666229305004,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "AKcL3l0h9X6I"
   },
   "outputs": [],
   "source": [
    "### 메인부 시작\n",
    "if __name__ == \"__main__\" :\n",
    "\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    import re\n",
    "\n",
    "    ### 유튜브 댓글\n",
    "    file_names = glob(\"./result/크롤링_유튜브댓글/*.csv\")\n",
    "    for file_name in file_names :\n",
    "        reviews = pd.read_csv(file_name, encoding='utf-8', sep=\",\")\n",
    "\n",
    "        #  전처리\n",
    "        documents = keywordAnalysis.preproc(reviews, custom_stopwords, custom_noun, spellcheck=False)\n",
    "        file_name = re.sub(r\".*\\\\\", \"\", file_name)  \n",
    "        file_name = re.sub(r\"\\.csv\", \"\", file_name)  # 파일 저장하기 편하려고...\n",
    "\n",
    "        #  빈도수 기반 키워드 추출\n",
    "        if file_name == \"뜨거운피_예고편\" :\n",
    "            cnt_th = 4  # 빈도수 4 이상만 추출\n",
    "        else :\n",
    "            cnt_th = 10  # 빈도수 10 이상만 추출\n",
    "        result_cnt = keywordAnalysis.cnt(documents, filename=f\"단어빈도수_유튜브_{file_name}\", cnt_th=cnt_th)\n",
    "\n",
    "\n",
    "        ### 이건 연구중...\n",
    "        \n",
    "        # #  그 중에서 빈도수 top10 키워드 목록 생성\n",
    "        # top10 = keywordAnalysis.cnt_top10(result_cnt)\n",
    "\n",
    "        # result_cnt[\"high_TFIDF_keywords\"] = [None for _ in range(len(result_cnt))]\n",
    "        # for keyword_cnt in top10 :\n",
    "        #     selected_docs = keywordAnalysis.sel_doc(documents, keyword_cnt)\n",
    "        #     result_tfidf = keywordAnalysis.tfidf(selected_docs)\n",
    "        #     result_tfidf = result_tfidf.head(5)\n",
    "\n",
    "        #     high_TFIDF_keywords = {}\n",
    "        #     for idx in range(len(result_tfidf)) :\n",
    "        #         high_TFIDF_keywords[result_tfidf.iloc[idx, 0]] = result_tfidf.iloc[idx, 2]\n",
    "\n",
    "        #     result_cnt.loc[keyword_cnt, \"high_TFIDF_keywords\"] = high_TFIDF_keywords\n",
    "        \n",
    "        # result_cnt.to_csv(f\"test_단어별TFIDF포함_{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1666229332866,
     "user": {
      "displayName": "hyoson heo",
      "userId": "18070894924019797882"
     },
     "user_tz": -540
    },
    "id": "KWOTiey49bQg"
   },
   "outputs": [],
   "source": [
    "    ### 네이버 영화\n",
    "\n",
    "    #  파일 한꺼번에 불러오기\n",
    "    file_names = glob(\"./result/크롤링_네이버영화/*.csv\")\n",
    "\n",
    "    for file_name in file_names :\n",
    "        reviews = pd.read_csv(file_name, encoding='utf-8', sep=\",\")\n",
    "\n",
    "        #  전처리\n",
    "        documents = keywordAnalysis.preproc(reviews, custom_stopwords, custom_noun, spellcheck=False)\n",
    "        file_name = re.sub(r\".*\\\\\", \"\", file_name)  # 파일 저장하기 편하려고...\n",
    "        file_name = re.sub(r\"csv\", \"\", file_name)  # 파일 저장하기 편하려고...\n",
    "\n",
    "        #  빈도수 기반 키워드 추출\n",
    "        cnt_th = 10\n",
    "        result_cnt_all = keywordAnalysis.cnt(documents, filename=f\"단어빈도수_네이버_{file_name}_전체\", cnt_th = cnt_th)    \n",
    "\n",
    "\n",
    "        ### 이건 연구중...\n",
    "        # #  빈도수 기반 키워드 추출 (높은 평점)\n",
    "        # result_cnt_hscore = keywordAnalysis.cnt(\n",
    "        #     documents[documents[\"score\"]>=8], f\"단어빈도수_{file_name}_평점8점이상\")    \n",
    "        # #  빈도수 기반 키워드 추출 (낮은 평점)\n",
    "        # result_cnt_lscore = keywordAnalysis.cnt(\n",
    "        #     documents[documents[\"score\"]<=4], f\"단어빈도수_{file_name}_평점4점이하\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvml8oD89dMI"
   },
   "outputs": [],
   "source": [
    "### 메인부 끝\n",
    "# ======================================================================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(cnt(\u001b[43ma\u001b[49m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m늑대사냥\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "print(cnt(a, filename='늑대사냥'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Word2Vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mWord2Vec\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Word2Vec'"
     ]
    }
   ],
   "source": [
    "import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNhJxnKWcmg373MZtxsTxG3",
   "collapsed_sections": [],
   "mount_file_id": "1hYNTZ3k0V5Ed5kD2DkiMjtU7n6YQOYc4",
   "provenance": [
    {
     "file_id": "1-V9kkZtt8dQqPgRloio7sewnyNaU2ybA",
     "timestamp": 1666228663695
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
