{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLwgKfGDvaXv"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcYHgCyXzBj2"
   },
   "outputs": [],
   "source": [
    "# 키워드 분석 클래스 시작\n",
    "class KeywordAnalysis() :\n",
    "\n",
    "    def __init__(self) :\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOcC0Z0BzO8E"
   },
   "outputs": [],
   "source": [
    "    ### 전처리 모듈\n",
    "    def preproc(self, reviews, custom_stopwords, custom_noun, spellcheck=False, stemming=False) :\n",
    "\n",
    "        print(\"전처리 시작\")\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import re\n",
    "        from ckonlpy.tag import Twitter\n",
    "        from hanspell import spell_checker\n",
    "        from tqdm import tqdm\n",
    "\n",
    "        #  문서군 변수 생성\n",
    "        documents = []\n",
    "\n",
    "        #  기존 불용어 위에 사용자 지정 불용어 적용\n",
    "        with open(\"./data/한국어불용어_new.txt\", \"r\", encoding=\"utf-8\") as f :\n",
    "            stop_words = f.read()\n",
    "        f.close()\n",
    "        stop_words = stop_words.split(\",\")\n",
    "        stop_words.extend(custom_stopwords)\n",
    "\n",
    "        #  사용자 지정 단어 사전 추가\n",
    "        from ckonlpy.tag import Twitter\n",
    "        custom_okt = Twitter()\n",
    "        for n in custom_noun:\n",
    "            custom_okt.add_dictionary(n, \"Noun\")\n",
    "\n",
    "        #  결측치가 존재하는 record 제거\n",
    "        reviews.dropna(inplace=True)\n",
    "\n",
    "        #  의미있는한글(ㅋㅋ, ㅎㅎ이런거 말고), 알파벳, 숫자, 띄어쓰기 제외한 글자 삭제\n",
    "        for idx, comment in enumerate(reviews[\"comment\"]) :\n",
    "            comment = re.sub(r\"[^가-힣a-zA-Z0-9 ]\", \"\", comment)\n",
    "            reviews.iloc[idx,2] = comment\n",
    "\n",
    "        #  띄어쓰기, 맞춤법 자동 교정 (갑자기 에러나서 일단 주석처리함)\n",
    "        # if spellcheck :\n",
    "        #     from hanspell import spell_checker\n",
    "        #     for comment in tqdm(reviews[\"comment\"]) :\n",
    "        #         comment = spell_checker.check(comment).checked\n",
    "        #         reviews.iloc[idx,2] = comment\n",
    "\n",
    "        #  토큰화 \n",
    "        \n",
    "        #  (with 불용어 제거, 한글자 단어 제거)\n",
    "        #  어간 추출하기\n",
    "        if stemming :\n",
    "            for comment in reviews[\"comment\"] :\n",
    "                document = []\n",
    "                words = custom_okt.morphs(comment, stem=True)\n",
    "                for word in words :\n",
    "                    if (len(word) >= 2) or (word in custom_noun) :\n",
    "                        if word not in stop_words :\n",
    "                            document.append(word)\n",
    "                documents.append(document)\n",
    "\n",
    "        #  명사 키워드만 추출하기 (디폴트)\n",
    "        else :  \n",
    "            for comment in reviews[\"comment\"] :\n",
    "                document = []\n",
    "                words = custom_okt.nouns(comment)\n",
    "                for word in words :\n",
    "                    if (len(word) >= 2) or (word in custom_noun) :\n",
    "                        if word not in stop_words :\n",
    "                            document.append(word)\n",
    "                documents.append(document)\n",
    "\n",
    "        #  문서군 변수 리턴\n",
    "        #  데이터 타입 : 1차원 리스트\n",
    "        #  데이터 구조 :\n",
    "        #  documents = [[토큰1, 토큰2, 토큰3, ...], [토큰1, 토큰2, 토큰3, ...], ...]\n",
    "        print(\"전처리 완료, 문서군 생성\\n\")\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rAVc12SzZdM"
   },
   "outputs": [],
   "source": [
    "    ### 카테고리별 단어등장한 문서개수 계산하는 모듈\n",
    "    def category_count(self, documents, filename=\"\") :\n",
    "\n",
    "        print(\"category_count 실행\\n\")\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        #  리턴할 변수 생성\n",
    "        result = None\n",
    "\n",
    "        #  카테고리 데이터프레임 불러오기\n",
    "        category = pd.read_csv(\"./data/카테고리 별 해당 키워드.csv\")\n",
    "\n",
    "        #  카운트한 값 & 전체 문서 개수가 들어갈 데이터프레임 생성\n",
    "        #  초기화\n",
    "        record_num = len(documents)\n",
    "        column_num = len(category.columns)\n",
    "        data = np.full((record_num, column_num+1), False)\n",
    "        #  column이름 지정\n",
    "        columns = category.columns.to_list()  # 카테고리 이름들\n",
    "        columns.append(\"전체문서개수\")         # 전체문서개수\n",
    "        #  데이터프레임 생성\n",
    "        cat_cnt = pd.DataFrame(data, columns = columns)\n",
    "\n",
    "\n",
    "        #  한 문서 안에 해당 카테고리의 단어가 존재하는지 체크\n",
    "        idx = 0\n",
    "        for document in documents :\n",
    "            # 전체 문서 개수 입력\n",
    "            cat_cnt.iloc[idx, 8] = True \n",
    "            # 해당 카테고리가 포함되어있으면 체크\n",
    "            for word in document :\n",
    "                if word in category[\"장르\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 0] = True\n",
    "\n",
    "                elif word in category[\"스토리/캐릭터명\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 1] = True\n",
    "\n",
    "                elif word in category[\"배우/연기\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 2] = True\n",
    "\n",
    "                elif word in category[\"감독\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 3] = True\n",
    "\n",
    "                elif word in category[\"영상미/연출\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 4] = True\n",
    "\n",
    "                elif word in category[\"OST/사운드\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 5] = True\n",
    "\n",
    "                elif word in category[\"감상\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 6] = True\n",
    "                    \n",
    "                elif word in category[\"기타\"].to_list() :\n",
    "                    cat_cnt.iloc[idx, 7] = True\n",
    "            idx += 1\n",
    "        \n",
    "\n",
    "        # 최종 결과 산출 후 리턴 : 각 카테고리의 단어가 언급된 문서들 개수를 계산\n",
    "        # 데이터 타입 : pd.Series\n",
    "        # (index : 카테고리 이름들, values : 각 카테고리 단어가 포함된 문서 개수)\n",
    "        result = cat_cnt.sum()\n",
    "\n",
    "        if filename != \"\" :\n",
    "            result.to_csv(f\"{filename}.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "            print(f\"{filename} 저장 완료!\")\n",
    "\n",
    "        print(\"category_count 종료\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vn1MF69-uT0i"
   },
   "outputs": [],
   "source": [
    "    ### 모든 결과들을 한꺼번에 비교하는 모듈\n",
    "    def compare_table(self, dict_list, filename=\"\") :\n",
    "        print(\"모든 분석 결과 비교 시작\")\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "        # dict_list리스트 안에 있던 딕셔너리들을 원래 데이터형으로 돌려놓기\n",
    "        # name_list : 영화 이름들 (str)\n",
    "        # ser_list : 각 영화마다의 분석 결과들 (pd.Serise)\n",
    "        name_list = [] \n",
    "        ser_list = []  \n",
    "        for dict in dict_list :\n",
    "            # 파일 이름 (딕셔너리key값 -> str 객체)\n",
    "            name = list(dict.keys())[0]\n",
    "            name_list.append(name)\n",
    "            # 분석결과물 (딕셔너리value값 -> pd.Series 객체)\n",
    "            ser = pd.Series(list(dict.values())[0])\n",
    "            ser_list.append(ser)\n",
    "\n",
    "        # 최종 결과물을 넣을 데이터프레임 생성\n",
    "        # * 필드값(data) : \"카테고리명 : 문서개수\"\n",
    "        # (즉, 어떤 카테고리의 단어가 몇개의 문서에서 나왔는지를 담음)\n",
    "        # * record 인덱스(index) : 영화 이름\n",
    "        # * columns : 순위\n",
    "        data = []\n",
    "        index = []\n",
    "        columns = []\n",
    "        for name in name_list :\n",
    "            index.append(name)\n",
    "        for idx in range(len(list(ser_list[0].index))-1) :\n",
    "            columns.append(f\"{idx+1}위\")\n",
    "        columns.append(\"전체문서개수\")\n",
    "\n",
    "        data = np.full((len(index), len(columns)), None)\n",
    "        result = pd.DataFrame(data, index=index, columns=columns)\n",
    "\n",
    "        # result 데이터프레임에 값 채워넣기\n",
    "        for idx1 in range(len(ser_list)) :\n",
    "            # 순위에 따라 내림차순으로 pd.serise객체 정렬\n",
    "            ser = ser_list[idx1].copy()\n",
    "            ser.drop(\"전체문서개수\", inplace=True)\n",
    "            ser.sort_values(ascending = False, inplace=True)\n",
    "            for idx2 in range(len(ser)) :\n",
    "                # pd.serise객체의 필드값들을 result 데이터프레임의 알맞은 위치에 입력\n",
    "                cat = ser.index[idx2]\n",
    "                num = ser.iat[idx2]\n",
    "                result.iat[idx1, idx2] = f\"{cat} : {num}개\"\n",
    "            # 전체문서개수 입력\n",
    "            ser = ser_list[idx1]\n",
    "            result.iat[idx1, 8] = ser.loc[\"전체문서개수\"]\n",
    "            print(result)\n",
    "\n",
    "        # 결과물 출력후 리턴\n",
    "        # 데이터 타입 : pd.DataFrame\n",
    "        if filename != \"\" :\n",
    "            result.to_csv(f\"{filename}.csv\", sep=\",\", encoding=\"utf-8\")\n",
    "            print(f\"{filename} 저장 완료!\")\n",
    "\n",
    "        print(\"집계 완료!\")\n",
    "        print(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ilPFGhuzZyk"
   },
   "outputs": [],
   "source": [
    "# 키워드 분석 모듈 끝\n",
    "# ---------------------------------------------------------------------------- #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKfdRGgrzmXs"
   },
   "outputs": [],
   "source": [
    "### 시각화 모듈 시작\n",
    "class Visualization() :\n",
    "    def __init__(self):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8geyU51jzmik"
   },
   "outputs": [],
   "source": [
    "    # 파이차트랑 막대그래프 동시에 그려주는 모듈\n",
    "    # (시각화 퀄리티가 구린건 나중에 손 볼 예정...)\n",
    "    def pie_n_bar(self, result, title) :\n",
    "\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        #  * 주의 : 이곳의 크기조절, 한글 글꼴설정은 VSCode용으로 제작된것.\n",
    "        #  colab이나 주피터노트북에서는 다른 설정을 적용해야함.\n",
    "        matplotlib.rcParams['figure.figsize'] = (12,7)\n",
    "        matplotlib.rcParams['font.family'] = \"NanumGothic\"\n",
    "        matplotlib.rcParams['font.size'] = 10   \n",
    "\n",
    "        result.drop(\"전체문서개수\", inplace=True)\n",
    "\n",
    "        #  제목\n",
    "        plt.suptitle(f\"해당 카테고리의 키워드가 나온 문서 비율 & 개수 : {title}\",fontsize=20)\n",
    "\n",
    "        #  파이차트 생성\n",
    "        #  (1행 2열 공간을 배정한 뒤, 첫번째 부분에 들어갈 그래프 생성)\n",
    "        plt.subplot(1, 2, 1)\n",
    "        \n",
    "        values = []\n",
    "        labels = []\n",
    "\n",
    "        values.extend(result.values.tolist())\n",
    "        labels.extend(result.index.to_list())\n",
    "        \n",
    "        wedgeprops={'width': 0.7, 'edgecolor': 'w', 'linewidth': 10}\n",
    "        plt.pie(\n",
    "            values, labels=labels,\n",
    "            startangle=90, autopct='%.1f%%', counterclock=False, wedgeprops=wedgeprops)\n",
    "        \n",
    "        #  막대그래프 생성\n",
    "        #  (1행 2열 공간을 배정한 뒤, 두번째 부분에 들어갈 그래프 생성)\n",
    "        plt.subplot(1, 2, 2) \n",
    "\n",
    "        values = []\n",
    "        labels = []\n",
    "\n",
    "        values.extend(result.values.tolist())\n",
    "        labels.extend(result.index.to_list())\n",
    "\n",
    "        plt.bar(\n",
    "            labels, values,\n",
    "            width=0.7)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.xlabel(\"키워드\", labelpad=11)\n",
    "        plt.ylabel(\"문서 개수\", labelpad=11)\n",
    "\n",
    "        plt.savefig(f\"시각화_{title}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndZIl8cQ0L8k"
   },
   "outputs": [],
   "source": [
    "# 시각화 클래스 끝 \n",
    "# ============================================================================ #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWbyjHwbzqEc"
   },
   "outputs": [],
   "source": [
    "### 전역변수부\n",
    "\n",
    "#  내가 직접 지정한 불용어 리스트 작성\n",
    "custom_stopwords = [\n",
    "    \"범죄도시\", \"범죄 도시\", \"범죄\", \"도시\", \"죽지\",\n",
    "    \"늑대사냥\", \"늑대\", \"사냥\",\n",
    "    \"공조\", \"인터네셔날\",\"인터내셔날\",\n",
    "    \"한산\", \"용\", \"용의\", \"출현\", \"용의출현\", \"장군님\", \"장군\",\n",
    "    \"뜨거운피\", \"피\", \"뜨거운\",\n",
    "    \"외계인\", \"외계\", \"인\",\n",
    "    \"영화\", \"무비\", \"영화관\", \"한국영화\",\n",
    "    \"진짜\", \"진자\", \"완전\", \"존나\", \"졸라\", \"느낌\", \"뭔가\", \"제일\", \"개\", \"보고\", \"사람\", \"보지\",\n",
    "    \"어요\", \"오늘\", \"그냥\", \"생각\", \"면서\", \"더니\", \"인적\", \"거지\", \"보기\", \"나름\", \"살짝\",\n",
    "    \"정말\", \"대박\", \"역대\", \"최고\", \"어제\", \"편이\", \"계속\", \"요소\", \"처럼\", \"이나\", \"역시\", \"부분\", \"던데\",\n",
    "    \"스포\", \"개봉\", \"한번\", \"내내\", \"구나\", \"때문\", \"어서\", \"정도\", \"다가\", \"다시\", \"누가\", \"덕분\", \"항상\",\n",
    "    \"봤는데\", \"왔는데\",\n",
    "    \"스포일러가 포함된 감상평입니다.\",\n",
    "    \"하나님\", \"영적\", \"그리스도\", \"성경\"\n",
    "]\n",
    "\n",
    "#  ckonlpy에 추가할 단어 지정\n",
    "custom_noun = [\n",
    "\n",
    "    #  [ 범죄도시 ]\n",
    "    \"장이수\", \"강해상\", \"손석구\", \"마석도\", \"마동석\", \"마블리\", \"박지환\", \"윤계상\",\n",
    "    \"2부\", \"2탄\", \"2편\", \"1부\", \"1탄\", \"1편\",\n",
    "    #  * 같은 단어 : 장이수=이수, 마석도=마동석=마블리\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 구씨 : 배우 손석구가 다른 작품에서 맡았던 케릭터 이름\n",
    "\n",
    "    #  [ 늑대사냥 ]\n",
    "    \"서인국\", \"장동윤\", \"박호산\", \"정소민\", \"고창석\", \"성동일\",\n",
    "    \"콘에어\", \"콘 에어\", \"잔인\",\n",
    "    #  * 같은 단어 : 콘에어=콘 에어\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 콘에어 : 니콜라스 케이지 출연 1997년 영화. 늑대사냥과 비슷한 소재를 다루고있음.\n",
    "\n",
    "    #  [ 공조2 ]\n",
    "    \"현빈\", \"유해진\", \"윤아\", \"다니엘 헤니\", \"다니엘헤니\", \"다니엘\", \"헤니\", \"진선규\",\n",
    "    \"장영남\", \"박훈\", \"임성재\", \"림청렬\", \"청렬\", \"강진태\", \"진태\", \"장명준\", \"박소연\",\n",
    "    \"박상위\", \"박민영\",\n",
    "    \"불시착\", \"사랑의 불시착\",\n",
    "    # 같은 단어 : 다니엘헤니=다니엘=헤니, 불시착=사랑의 불시착\n",
    "\n",
    "    #  [ 외계인1부 ]\n",
    "    \"류준열\", \"김우빈\", \"김태리\", \"소지섭\", \"염정아\", \"조우진\", \"이하늬\",\n",
    "    \"무륵\", \"김현중\", \"이안\", \"문도석\", \"흑설\", \"청운\",\n",
    "    \"2부\", \"2탄\", \"2편\", \"1부\", \"1탄\", \"1편\", \"돈\", \"SF\", \"sf\", \"SF물\", \"sf물\",\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 돈 : 한 글자 '돈 아깝다'같은 말이 종종 보임.\n",
    "    #  - SF, sf : 장르명. 한글이 아니라고해서 없애면 안됨.\n",
    "\n",
    "    #  [ 한산 ]\n",
    "    \"박해일\", \"안성기\", \"변요한\", \"손현주\", \"김성규\", \"김성균\", \"김향기\", \"택연\", \"옥택연\",\n",
    "    \"이순신\", \"장군님\", \"장군\", \"어영담\", \"원균\", \"준산\", \"히데요시\", \"도요토미\", \"도요토미 히데요시\",\n",
    "    \"한산도\", \"대첩\", \"한산도 대첩\", \"임진왜란\", \"유키나가\", \"학익진\",\n",
    "    #  * 같은 단어 : 이순신=이순신장군\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 하라 : '전군 출정하라', '선회하라', '발포하라'가 이 영화의 명대사인가봄.\n",
    "\n",
    "    # 뜨거운피 배우명/등장인물명\n",
    "    \"정우\", \"김갑수\", \"최무성\", \"지승현\", \"김해곤\", \"윤지혜\", \"이홍내\", \"정호빈\"\n",
    "    #  * 불용어 처리하면 안되는 단어 :\n",
    "    #  - 짱구 : 배우 정우가 다른 작품에서 맡았던 케릭터 이름\n",
    "\n",
    "    ]\n",
    "\n",
    "#  클래스 생성자 만들기\n",
    "keywordAnalysis = KeywordAnalysis()\n",
    "visualization = Visualization()\n",
    "\n",
    "#  전체 집계를 위해, 카운트 결과들을 한데 모을 변수\n",
    "#  데이터 타입 : 딕셔너리형 데이터들을 원소로 가지는 리스트\n",
    "#  [{파일이름1 : pd.Series데이터1}, {파일이름2 : pd.Series데이터2}, ...]\n",
    "cat_cnt_df_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SracX0gizqQ7"
   },
   "outputs": [],
   "source": [
    "# 전역변수부 끝\n",
    "# ============================================================================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96dwQRYUuqeR"
   },
   "outputs": [],
   "source": [
    "### 메인부\n",
    "if __name__ == \"__main__\" :\n",
    "    from glob import glob\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ku6O3cPD0Y0y"
   },
   "outputs": [],
   "source": [
    "    ### 유튜브 댓글\n",
    "\n",
    "    # 폴더 안에 있는 개별 파일마다 작업 수행\n",
    "    file_names = glob(\"./result/크롤링_유튜브댓글/*.csv\")\n",
    "    for file_name in file_names :\n",
    "\n",
    "        # 크롤링한 결과물 csv 파일 불러오기\n",
    "        reviews = pd.read_csv(file_name, encoding='utf-8', sep=\",\")\n",
    "\n",
    "        # 전처리 작업, 토큰화\n",
    "        documents = keywordAnalysis.preproc(reviews, custom_stopwords, custom_noun)\n",
    "\n",
    "        # 그냥 파일 저장하거나 그래프 제목달때 편하라고 한것\n",
    "        file_name = re.sub(r\".*\\\\\", \"\", file_name)  \n",
    "        file_name = re.sub(r\"\\.csv\", \"\", file_name) \n",
    "\n",
    "        # 각 카테고리별로 문서 개수 카운트\n",
    "        cat_cnt_df = keywordAnalysis.category_count(documents, filename=file_name)\n",
    "\n",
    "        # 전체 집계를 위해, 카운트 결과물을 cat_cnt_df_list에 차례로 저장\n",
    "        cat_cnt_df_list.append({file_name:cat_cnt_df})\n",
    "\n",
    "        # 위의 분석 결과를 파이차트와 막대그래프로 시각화\n",
    "        visualization.pie_n_bar(cat_cnt_df, file_name)\n",
    "\n",
    "    # 전체 집계 실시\n",
    "    keywordAnalysis.compare_table(cat_cnt_df_list, filename=\"유튜브댓글_전체비교\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gEpKWfC8uwsY"
   },
   "outputs": [],
   "source": [
    "    ### 네이버 영화\n",
    "\n",
    "    # 전체집계용 변수 초기화\n",
    "    cat_cnt_df_list = []\n",
    "\n",
    "    # 폴더 안에 있는 개별 파일마다 작업 수행\n",
    "    file_names = glob(\"./result/크롤링_네이버영화/*.csv\")\n",
    "    for file_name in file_names :\n",
    "\n",
    "        # 크롤링한 결과물 csv 파일 불러오기\n",
    "        reviews = pd.read_csv(file_name, encoding='utf-8', sep=\",\")\n",
    "\n",
    "        # 전처리 작업, 토큰화\n",
    "        documents = keywordAnalysis.preproc(reviews, custom_stopwords, custom_noun)\n",
    "\n",
    "        # 그냥 파일 저장하거나 그래프 제목달때 편하라고 한것\n",
    "        file_name = re.sub(r\".*\\\\\", \"\", file_name)  \n",
    "        file_name = re.sub(r\"\\.csv\", \"\", file_name) \n",
    "\n",
    "        # 각 카테고리별로 문서 개수 세기\n",
    "        cat_cnt_df = keywordAnalysis.category_count(documents, filename=file_name)\n",
    "\n",
    "        # 전체 집계를 위해, 카운트 결과물을 cat_cnt_df_list에 차례로 저장\n",
    "        cat_cnt_df_list.append({file_name:cat_cnt_df})\n",
    "\n",
    "        # # 위의 분석 결과를 파이차트와 막대그래프로 시각화\n",
    "        visualization.pie_n_bar(cat_cnt_df, file_name)\n",
    "\n",
    "    # 전체 집계 실시\n",
    "    keywordAnalysis.compare_table(cat_cnt_df_list, filename=\"네이버영화_전체비교\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHv5Xq150ZEi"
   },
   "outputs": [],
   "source": [
    "# 메인부 끝\n",
    "# ============================================================================ #"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNC2HvE3rofywezGrLfx1tZ",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
